# -*- coding: utf-8 -*-
"""web_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nwXeUCb39fAQ8vRnvPLlBdXHTEwhYEx2
"""

import requests
url='https://groceries.aldi.co.uk/en-GB/Search?keywords=butter'
response = requests.get(url)
print(response)
from bs4 import BeautifulSoup
soup = BeautifulSoup(response.text, 'html.parser')
print(soup.title)

supermarkets = [
    {
      'shop': 'Tesco',
      'item': {'element': 'li', 'class':'product-list--list-item'},
      'name': {'element': 'span', 'class':'ldbwMG'},
      'price': {'element': 'p', 'class':'jWPEtj'},
      'url': 'https://www.tesco.com/groceries/en-GB/search?query='
    },
    {
      'shop': 'M&S',
      'item': {'element': 'li', 'class':'fops-item'},
      'name': {'element': 'h4', 'class':'fop-title'},
      'price': {'element': 'p', 'class':'fop-price'},
      'url': 'https://www.ocado.com/search?entry='
    }, 
    {
      'shop': 'Morrisons',
      'item': {'element': 'div', 'class':'fop-item'},
      'name': {'element': 'h4', 'class':'fop-title'},
      'price': {'element': 'span', 'class':'fop-price'},
      'url': 'https://groceries.morrisons.com/search?entry='
    },
    {
      'shop': 'Iceland',
      'item': {'element': 'li', 'class':'grid-tile'},
      'name': {'element': 'a', 'class':'name-link'},
      'price': {'element': 'span', 'class':'product-sales-price'},
      'url': 'https://groceries.morrisons.com/search?entry='
    },
    {
      'shop': 'Waitrose',
      'item': {'element': 'div', 'class':'content___3kznT'},
      'name': {'element': 'span', 'class':'name___h83Rn'},
      'price': {'element': 'span', 'class':'itemPrice___ieIBH'},
      'url': 'https://www.waitrose.com/ecom/shop/search?&searchTerm='
    }
    ]
  
products = ['bread', 'butter', 'milk', 'eggs']

# Web Scraping for Tesco
import requests
from bs4 import BeautifulSoup

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.67 Safari/537.36 Edg/87.0.664.47','Connection': 'keep-alive','Cache-Control': 'max-age=0','Upgrade-Insecure-Requests': '1',
'Accept-Encoding': 'gzip, deflate',
'Accept-Language': 'en-US,en;q=0.9',
'Cookie': 'AuthSession=dddddd'}

shop = supermarkets[2]
results = []

for product in products:
  r = requests.get(shop['url']+product, headers=HEADERS)

  soup = BeautifulSoup(r.text, 'html.parser')
  print(soup.title)
  prices = []
  items = soup.findAll(shop['item']['element'], attrs={"class":shop['item']['class']})
  print(items)
  for item in items:
      name = item.find(shop['name']['element'], class_=shop['name']['class'])
      price = item.find(shop['price']['element'], class_=shop['price']['class'])

      if name and price:
        print(name.text)
        print(price.text)
        # print(float(price.text.replace('£','')))
        if 'p' in price.text:
          print('0.'+price.text.replace('p',''))
          prices.append(float('0.'+price.text.replace('p','')))
        else:
          prices.append(float(price.text.replace('£',''))) 
        # print('\n')
  print(prices)
  # print('Average: '+str(round(sum(prices)/len(prices),2)))
  results.append({product, str(round(sum(prices)/len(prices),2))})

print(results)

import requests

response = requests.get(
  url='https://proxy.scrapeops.io/v1/',
  params={
      'api_key': 'c17b7c28-a3dc-46d4-b904-382e6e910362',
      'url': 'https://groceries.asda.com/search/butter', 
      'headers' : {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}
  },
)

soup = BeautifulSoup(response.text, 'html.parser')
print(soup.title)

items = soup.findAll('a', attrs={"class":"co-product__anchor"})
print(items)

# Google Maps API Nearby Supermarkets

import requests

url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=51.609929%2C0.109067&radius=1500&type=convenience_store&key=AIzaSyAAOpsLqUS8AFih-Fp2QgeldirT-Eoc0zg"

payload={}
headers = {}

response = requests.request("GET", url, headers=headers, data=payload)

print(response.text)
